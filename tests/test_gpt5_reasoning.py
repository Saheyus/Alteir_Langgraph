#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Tests de configuration GPT-5 vs GPT-4

V√©rifie que :
- GPT-5 utilise reasoning au lieu de temperature
- GPT-4 utilise temperature classique
- Les param√®tres sont correctement pass√©s √† l'API
"""
import os
import sys
import pytest
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

# Fix encoding pour Windows
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

load_dotenv()

@pytest.mark.slow
def test_gpt5_reasoning():
    """Test GPT-5 avec reasoning"""
    print("\n" + "="*60)
    print("üß† TEST GPT-5 AVEC REASONING")
    print("="*60)
    
    llm = ChatOpenAI(
        model="gpt-5-nano",
        use_responses_api=True,
        reasoning={"effort": "minimal"},  # minimal pour question simple
        max_tokens=2000,  # Plus de tokens pour avoir de la place pour la r√©ponse
    )
    
    prompt = "Quelle est la capitale de la France ? Explique ton raisonnement."
    
    print(f"\nüìù Prompt: {prompt}")
    print("\n‚è≥ Appel API en cours...")
    
    response = llm.invoke(prompt)
    
    # Afficher TOUT l'objet response pour debug
    print("\nüîç DEBUG COMPLET DE LA R√âPONSE:")
    print(f"  - Type: {type(response)}")
    print(f"  - Content: {response.content}")
    print(f"  - Dir: {[attr for attr in dir(response) if not attr.startswith('_')][:20]}")
    
    # Afficher le texte de r√©ponse
    print("\n‚úÖ R√âPONSE TEXTE:")
    if hasattr(response, 'text'):
        print(response.text)
    else:
        print(response.content)
    
    # Afficher les content_blocks pour voir le reasoning
    print("\nüîç CONTENT BLOCKS:")
    if hasattr(response, 'content_blocks') and response.content_blocks:
        for i, block in enumerate(response.content_blocks):
            print(f"\n--- Block {i+1} ---")
            print(f"Type: {block.get('type', 'unknown')}")
            print(f"Contenu complet: {block}")
    else:
        print("‚ö†Ô∏è Pas de content_blocks")
        
    # V√©rifier additional_kwargs
    if hasattr(response, 'additional_kwargs'):
        print("\nüì¶ ADDITIONAL_KWARGS:")
        print(response.additional_kwargs)
    
    # Afficher les m√©tadonn√©es
    print("\nüìä METADATA:")
    print(f"  - Model: {response.response_metadata.get('model_name', 'N/A')}")
    print(f"  - Tokens: {response.response_metadata.get('token_usage', 'N/A')}")
    
    return response


@pytest.mark.slow
def test_gpt4_temperature():
    """Test GPT-4 avec temperature (sans reasoning)"""
    print("\n" + "="*60)
    print("üå°Ô∏è TEST GPT-4 AVEC TEMPERATURE")
    print("="*60)
    
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.7,
        max_tokens=1000,
    )
    
    prompt = "Quelle est la capitale de la France ? Explique ton raisonnement."
    
    print(f"\nüìù Prompt: {prompt}")
    print("\n‚è≥ Appel API en cours...")
    
    response = llm.invoke(prompt)
    
    # Afficher le texte de r√©ponse
    print("\n‚úÖ R√âPONSE TEXTE:")
    print(response.content)
    
    # V√©rifier les content_blocks (ne devrait pas avoir de reasoning)
    print("\nüîç CONTENT BLOCKS:")
    if hasattr(response, 'content_blocks'):
        for i, block in enumerate(response.content_blocks):
            print(f"\n--- Block {i+1} (type: {block.get('type', 'unknown')}) ---")
            print(block)
    else:
        print("‚úÖ Pas de content_blocks (normal pour GPT-4 classique)")
    
    # Afficher les m√©tadonn√©es
    print("\nüìä METADATA:")
    print(f"  - Model: {response.response_metadata.get('model_name', 'N/A')}")
    print(f"  - Tokens: {response.response_metadata.get('token_usage', 'N/A')}")
    
    return response


def compare_configs():
    """Compare les configs LLM"""
    print("\n" + "="*60)
    print("‚öôÔ∏è COMPARAISON DES CONFIGURATIONS")
    print("="*60)
    
    # Config GPT-5
    print("\nüöÄ GPT-5 (reasoning):")
    llm_gpt5 = ChatOpenAI(
        model="gpt-5-nano",
        use_responses_api=True,
        reasoning={"effort": "medium"},
        max_tokens=1000,
    )
    print(f"  - Model: {llm_gpt5.model_name}")
    print(f"  - Uses responses API: {getattr(llm_gpt5, 'use_responses_api', False)}")
    print(f"  - Temperature: {getattr(llm_gpt5, 'temperature', 'N/A')}")
    
    # Config GPT-4
    print("\nüîÑ GPT-4 (temperature):")
    llm_gpt4 = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.7,
        max_tokens=1000,
    )
    print(f"  - Model: {llm_gpt4.model_name}")
    print(f"  - Uses responses API: {getattr(llm_gpt4, 'use_responses_api', False)}")
    print(f"  - Temperature: {llm_gpt4.temperature}")


@pytest.mark.unit
def test_compare_configs():
    """Test de comparaison des configurations (sans appel API)"""
    compare_configs()


if __name__ == "__main__":
    # Permet de lancer le script directement pour debug
    print("\nüß™ TEST DES PARAM√àTRES GPT-5 vs GPT-4")
    print("="*60)
    print("üí° Pour lancer via pytest : pytest tests/test_gpt5_reasoning.py -v")
    print("üí° Pour tests rapides uniquement : pytest tests/test_gpt5_reasoning.py -m unit")
    print("="*60)
    
    # V√©rifier la cl√© API
    if not os.getenv("OPENAI_API_KEY"):
        print("‚ùå ERREUR: OPENAI_API_KEY non d√©finie dans .env")
        exit(1)
    
    try:
        # Comparer les configs
        compare_configs()
        
        # Test GPT-5
        response_gpt5 = test_gpt5_reasoning()
        
        # Test GPT-4
        response_gpt4 = test_gpt4_temperature()
        
        print("\n" + "="*60)
        print("‚úÖ TESTS TERMIN√âS")
        print("="*60)
        print("\nüí° Points v√©rifi√©s:")
        print("  1. GPT-5 utilise use_responses_api=True")
        print("  2. GPT-5 utilise reasoning au lieu de temperature")
        print("  3. GPT-4 utilise temperature classique")
        
    except Exception as e:
        print(f"\n‚ùå ERREUR: {e}")
        import traceback
        traceback.print_exc()

