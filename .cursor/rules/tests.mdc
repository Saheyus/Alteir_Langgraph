---
description: for all tests
alwaysApply: false
---
# Tests - Bonnes Pratiques

## ğŸ“ Organisation des Tests

### Structure des dossiers
```
tests/
â”œâ”€â”€ test_notion_api_2025.py
â”œâ”€â”€ test_agents.py
â”œâ”€â”€ test_workflows.py
â”œâ”€â”€ test_gpt5_reasoning.py  â† Tests de configuration LLM
â””â”€â”€ ...
```

### âš ï¸ RÃˆGLE CRITIQUE : NE JAMAIS crÃ©er de fichiers de test Ã  la racine

**âŒ INTERDIT :**
```
test_*.py  â† Ã€ la racine du projet
*_test.py  â† Ã€ la racine du projet
```

**âœ… OBLIGATOIRE :**
```
tests/test_*.py  â† Dans le dossier tests/
```

## ğŸ§ª Types de Tests

### 1. Tests unitaires d'agents
**Fichier :** `tests/test_agents.py`
```python
"""Tests unitaires des agents (Writer, Reviewer, etc.)"""
import pytest
from agents.writer_agent import WriterAgent
from agents.base.domain_config import DomainConfig

def test_writer_agent_initialization():
    """Test l'initialisation du WriterAgent"""
    # ...

def test_writer_agent_process():
    """Test la mÃ©thode process du WriterAgent"""
    # ...
```

### 2. Tests d'intÃ©gration workflow
**Fichier :** `tests/test_workflows.py`
```python
"""Tests d'intÃ©gration des workflows LangGraph"""
from workflows.content_workflow import ContentWorkflow

def test_complete_workflow():
    """Test le workflow complet de gÃ©nÃ©ration"""
    # ...
```

### 3. Tests de configuration LLM
**Fichier :** `tests/test_gpt5_reasoning.py`
```python
"""Tests de configuration GPT-5 vs GPT-4"""
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

def test_gpt5_config():
    """VÃ©rifie que GPT-5 utilise reasoning"""
    # ...

def test_gpt4_config():
    """VÃ©rifie que GPT-4 utilise temperature"""
    # ...
```

### 4. Tests API Notion
**Fichier :** `tests/test_notion_api_2025.py` (dÃ©jÃ  existant)
```python
"""Tests de l'API Notion 2025-09-03"""
# Tests existants...
```

## ğŸš€ Lancer les Tests

### Tous les tests
```bash
pytest tests/
```

### Un fichier spÃ©cifique
```bash
pytest tests/test_agents.py
```

### Un test spÃ©cifique
```bash
pytest tests/test_agents.py::test_writer_agent_process
```

### Avec verbose
```bash
pytest tests/ -v
```

### Avec coverage
```bash
pytest tests/ --cov=agents --cov=workflows --cov-report=html
```

## ğŸ“ Conventions de Nommage

### Fichiers
- PrÃ©fixe : `test_`
- Nom descriptif : `test_<module>_<aspect>.py`
- Exemples : `test_agents.py`, `test_notion_api.py`, `test_gpt5_reasoning.py`

### Fonctions de test
- PrÃ©fixe : `test_`
- Nom descriptif : `test_<ce_qui_est_testÃ©>`
- Exemples : `test_writer_agent_initialization()`, `test_gpt5_uses_reasoning()`

### Classes de test
- PrÃ©fixe : `Test`
- Nom descriptif : `Test<Module>`
- Exemples : `TestWriterAgent`, `TestNotionAPI`

## âš™ï¸ Configuration pytest

**Fichier :** `pytest.ini` (Ã  crÃ©er si besoin)
```ini
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = 
    -v
    --tb=short
    --strict-markers
markers =
    slow: Tests lents (nÃ©cessitent des appels API)
    unit: Tests unitaires (rapides, pas d'API)
    integration: Tests d'intÃ©gration
```

## ğŸ·ï¸ Markers pytest

### Utilisation
```python
import pytest

@pytest.mark.unit
def test_fast_function():
    """Test rapide sans appel API"""
    pass

@pytest.mark.slow
def test_api_call():
    """Test lent avec appel API"""
    pass

@pytest.mark.integration
def test_complete_workflow():
    """Test d'intÃ©gration complet"""
    pass
```

### Lancer par marker
```bash
pytest -m unit        # Seulement les tests unitaires
pytest -m "not slow"  # Exclure les tests lents
pytest -m integration # Seulement les tests d'intÃ©gration
```

## ğŸ”§ Fixtures Communes

**Fichier :** `tests/conftest.py`
```python
"""Fixtures pytest communes Ã  tous les tests"""
import pytest
from langchain_openai import ChatOpenAI

@pytest.fixture
def mock_llm():
    """Mock LLM pour tests sans appel API"""
    # ...

@pytest.fixture
def domain_config():
    """Config de domaine pour tests"""
    # ...

@pytest.fixture
def sample_brief():
    """Brief d'exemple pour tests"""
    return "Un alchimiste qui transforme les Ã©motions..."
```

## ğŸ“Š Tests de Performance

**Fichier :** `tests/test_performance.py`
```python
"""Tests de performance et benchmarks"""
import time
import pytest

@pytest.mark.slow
def test_workflow_execution_time():
    """VÃ©rifie que le workflow s'exÃ©cute en <60s"""
    start = time.time()
    # ... exÃ©cuter workflow
    duration = time.time() - start
    assert duration < 60, f"Workflow trop lent: {duration}s"
```

## ğŸ› Tests de Debug

Pour des tests de debug **temporaires** :
- CrÃ©er dans `tests/debug/` (ajouter au .gitignore si nÃ©cessaire)
- **Toujours nettoyer** aprÃ¨s debug
- Ne **JAMAIS** commit des tests de debug

## âœ… Checklist avant Commit

- [ ] Tous les fichiers de test sont dans `tests/`
- [ ] Aucun fichier `test_*.py` Ã  la racine
- [ ] Tests passent avec `pytest tests/` (OBLIGATOIRE â€” exÃ©cutÃ© par le dev; coller la sortie si pertinent)
- [ ] Coverage acceptable (optionnel)
- [ ] Pas de tests de debug temporaires committÃ©s

## ğŸ¯ Exemples Pratiques

### Test rapide d'une config LLM
```bash
# CrÃ©er le test dans tests/
cat > tests/test_my_feature.py << 'EOF'
def test_my_feature():
    assert True
EOF

# Lancer
pytest tests/test_my_feature.py

# Si c'Ã©tait temporaire, supprimer
rm tests/test_my_feature.py
```

### Test avec appel API (marquÃ© slow)
```python
# tests/test_gpt5_reasoning.py
import pytest
from langchain_openai import ChatOpenAI

@pytest.mark.slow
def test_gpt5_reasoning():
    """VÃ©rifie le reasoning de GPT-5 (nÃ©cessite API key)"""
    llm = ChatOpenAI(
        model="gpt-5-nano",
        use_responses_api=True,
        reasoning={"effort": "minimal"},
    )
    response = llm.invoke("Test")
    assert 'reasoning' in response.additional_kwargs
```

## ğŸ“š Ressources

- [pytest documentation](https://docs.pytest.org/)
- [pytest fixtures](https://docs.pytest.org/en/stable/fixture.html)
- [pytest markers](https://docs.pytest.org/en/stable/mark.html)
# Tests - Bonnes Pratiques

## ğŸ“ Organisation des Tests

### Structure des dossiers
```
tests/
â”œâ”€â”€ test_notion_api_2025.py
â”œâ”€â”€ test_agents.py
â”œâ”€â”€ test_workflows.py
â”œâ”€â”€ test_gpt5_reasoning.py  â† Tests de configuration LLM
â””â”€â”€ ...
```

### âš ï¸ RÃˆGLE CRITIQUE : NE JAMAIS crÃ©er de fichiers de test Ã  la racine

**âŒ INTERDIT :**
```
test_*.py  â† Ã€ la racine du projet
*_test.py  â† Ã€ la racine du projet
```

**âœ… OBLIGATOIRE :**
```
tests/test_*.py  â† Dans le dossier tests/
```

## ğŸ§ª Types de Tests

### 1. Tests unitaires d'agents
**Fichier :** `tests/test_agents.py`
```python
"""Tests unitaires des agents (Writer, Reviewer, etc.)"""
import pytest
from agents.writer_agent import WriterAgent
from agents.base.domain_config import DomainConfig

def test_writer_agent_initialization():
    """Test l'initialisation du WriterAgent"""
    # ...

def test_writer_agent_process():
    """Test la mÃ©thode process du WriterAgent"""
    # ...
```

### 2. Tests d'intÃ©gration workflow
**Fichier :** `tests/test_workflows.py`
```python
"""Tests d'intÃ©gration des workflows LangGraph"""
from workflows.content_workflow import ContentWorkflow

def test_complete_workflow():
    """Test le workflow complet de gÃ©nÃ©ration"""
    # ...
```

### 3. Tests de configuration LLM
**Fichier :** `tests/test_gpt5_reasoning.py`
```python
"""Tests de configuration GPT-5 vs GPT-4"""
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI

def test_gpt5_config():
    """VÃ©rifie que GPT-5 utilise reasoning"""
    # ...

def test_gpt4_config():
    """VÃ©rifie que GPT-4 utilise temperature"""
    # ...
```

### 4. Tests API Notion
**Fichier :** `tests/test_notion_api_2025.py` (dÃ©jÃ  existant)
```python
"""Tests de l'API Notion 2025-09-03"""
# Tests existants...
```

## ğŸš€ Lancer les Tests

### Tous les tests
```bash
pytest tests/
```

### Un fichier spÃ©cifique
```bash
pytest tests/test_agents.py
```

### Un test spÃ©cifique
```bash
pytest tests/test_agents.py::test_writer_agent_process
```

### Avec verbose
```bash
pytest tests/ -v
```

### Avec coverage
```bash
pytest tests/ --cov=agents --cov=workflows --cov-report=html
```

## ğŸ“ Conventions de Nommage

### Fichiers
- PrÃ©fixe : `test_`
- Nom descriptif : `test_<module>_<aspect>.py`
- Exemples : `test_agents.py`, `test_notion_api.py`, `test_gpt5_reasoning.py`

### Fonctions de test
- PrÃ©fixe : `test_`
- Nom descriptif : `test_<ce_qui_est_testÃ©>`
- Exemples : `test_writer_agent_initialization()`, `test_gpt5_uses_reasoning()`

### Classes de test
- PrÃ©fixe : `Test`
- Nom descriptif : `Test<Module>`
- Exemples : `TestWriterAgent`, `TestNotionAPI`

## âš™ï¸ Configuration pytest

**Fichier :** `pytest.ini` (Ã  crÃ©er si besoin)
```ini
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = 
    -v
    --tb=short
    --strict-markers
markers =
    slow: Tests lents (nÃ©cessitent des appels API)
    unit: Tests unitaires (rapides, pas d'API)
    integration: Tests d'intÃ©gration
```

## ğŸ·ï¸ Markers pytest

### Utilisation
```python
import pytest

@pytest.mark.unit
def test_fast_function():
    """Test rapide sans appel API"""
    pass

@pytest.mark.slow
def test_api_call():
    """Test lent avec appel API"""
    pass

@pytest.mark.integration
def test_complete_workflow():
    """Test d'intÃ©gration complet"""
    pass
```

### Lancer par marker
```bash
pytest -m unit        # Seulement les tests unitaires
pytest -m "not slow"  # Exclure les tests lents
pytest -m integration # Seulement les tests d'intÃ©gration
```

## ğŸ”§ Fixtures Communes

**Fichier :** `tests/conftest.py`
```python
"""Fixtures pytest communes Ã  tous les tests"""
import pytest
from langchain_openai import ChatOpenAI

@pytest.fixture
def mock_llm():
    """Mock LLM pour tests sans appel API"""
    # ...

@pytest.fixture
def domain_config():
    """Config de domaine pour tests"""
    # ...

@pytest.fixture
def sample_brief():
    """Brief d'exemple pour tests"""
    return "Un alchimiste qui transforme les Ã©motions..."
```

## ğŸ“Š Tests de Performance

**Fichier :** `tests/test_performance.py`
```python
"""Tests de performance et benchmarks"""
import time
import pytest

@pytest.mark.slow
def test_workflow_execution_time():
    """VÃ©rifie que le workflow s'exÃ©cute en <60s"""
    start = time.time()
    # ... exÃ©cuter workflow
    duration = time.time() - start
    assert duration < 60, f"Workflow trop lent: {duration}s"
```

## ğŸ› Tests de Debug

Pour des tests de debug **temporaires** :
- CrÃ©er dans `tests/debug/` (ajouter au .gitignore si nÃ©cessaire)
- **Toujours nettoyer** aprÃ¨s debug
- Ne **JAMAIS** commit des tests de debug

## âœ… Checklist avant Commit

- [ ] Tous les fichiers de test sont dans `tests/`
- [ ] Aucun fichier `test_*.py` Ã  la racine
- [ ] Tests passent avec `pytest tests/`
- [ ] Coverage acceptable (optionnel)
- [ ] Pas de tests de debug temporaires committÃ©s

## ğŸ¯ Exemples Pratiques

### Test rapide d'une config LLM
```bash
# CrÃ©er le test dans tests/
cat > tests/test_my_feature.py << 'EOF'
def test_my_feature():
    assert True
EOF

# Lancer
pytest tests/test_my_feature.py

# Si c'Ã©tait temporaire, supprimer
rm tests/test_my_feature.py
```

### Test avec appel API (marquÃ© slow)
```python
# tests/test_gpt5_reasoning.py
import pytest
from langchain_openai import ChatOpenAI

@pytest.mark.slow
def test_gpt5_reasoning():
    """VÃ©rifie le reasoning de GPT-5 (nÃ©cessite API key)"""
    llm = ChatOpenAI(
        model="gpt-5-nano",
        use_responses_api=True,
        reasoning={"effort": "minimal"},
    )
    response = llm.invoke("Test")
    assert 'reasoning' in response.additional_kwargs
```

## ğŸ“š Ressources

- [pytest documentation](https://docs.pytest.org/)
- [pytest fixtures](https://docs.pytest.org/en/stable/fixture.html)
- [pytest markers](https://docs.pytest.org/en/stable/mark.html)
