# ğŸ”„ Guide Multi-Fournisseur LLM

Ce guide explique comment utiliser le systÃ¨me avec diffÃ©rents fournisseurs LLM (OpenAI, Anthropic, Mistral, Ollama, etc.).

## ğŸ¯ StratÃ©gie

### Architecture Agnostique

Le systÃ¨me utilise **`LLMAdapter`** pour abstraire les diffÃ©rences entre fournisseurs :

```python
from agents.base.llm_utils import LLMAdapter

# Fonctionne avec n'importe quel LLM
adapter = LLMAdapter(llm)

# Obtenir une sortie structurÃ©e
result = adapter.get_structured_output(
    prompt="Corrige ce texte...",
    schema=CorrectionResult
)
```

### 3 Niveaux de Fallback

1. **Structured Outputs natifs** (OpenAI, Anthropic) â†’ JSON garanti
2. **JSON mode** (Mistral, Ollama) â†’ Parsing JSON du texte
3. **Parser manuel** (fallback) â†’ Regex/extraction custom

---

## ğŸ“¦ Fournisseurs SupportÃ©s

### 1. **OpenAI** (RecommandÃ© pour production)

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model="gpt-5-nano",  # Rapide et Ã©conomique
    use_responses_api=True,
    extra_body={
        "reasoning": {"effort": "minimal"},
        "max_output_tokens": 1000,
    }
)
```

**Avantages :**
- âœ… Structured Outputs natifs (100% fiable)
- âœ… GPT-5-nano trÃ¨s rapide
- âœ… Reasoning intÃ©grÃ©
- âŒ NÃ©cessite clÃ© API payante

### 2. **Anthropic Claude**

```python
from langchain_anthropic import ChatAnthropic

llm = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",
    temperature=0.3
)
```

**Avantages :**
- âœ… Excellent en franÃ§ais
- âœ… JSON mode fiable
- âœ… Contexte 200k tokens
- âŒ Plus cher que GPT-5-nano

### 3. **Mistral AI**

```python
from langchain_mistralai import ChatMistralAI

llm = ChatMistralAI(
    model="mistral-large-latest",
    temperature=0.3
)
```

**Avantages :**
- âœ… FranÃ§ais natif
- âœ… Bon rapport qualitÃ©/prix
- âš ï¸ JSON mode, parsing nÃ©cessaire

### 4. **Ollama** (Local, gratuit)

```python
from langchain_ollama import ChatOllama

llm = ChatOllama(
    model="llama3.2",  # ou mistral, qwen, etc.
    temperature=0.3,
    format="json"
)
```

**Avantages :**
- âœ… Totalement gratuit et local
- âœ… Pas de limite de requÃªtes
- âœ… ConfidentialitÃ© garantie
- âŒ QualitÃ© moindre que GPT-5
- âŒ Plus lent (dÃ©pend du GPU)

---

## ğŸ”§ Configuration par Environnement

### Fichier `.env`

```bash
# Production : OpenAI
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-...

# Dev : Anthropic
# LLM_PROVIDER=anthropic
# ANTHROPIC_API_KEY=sk-ant-...

# Local : Ollama
# LLM_PROVIDER=ollama
# OLLAMA_BASE_URL=http://localhost:11434
```

### Code Adaptatif

```python
import os
from agents.base.llm_utils import LLMAdapter

def create_llm():
    """CrÃ©e le LLM selon l'environnement"""
    provider = os.getenv("LLM_PROVIDER", "openai")
    
    if provider == "openai":
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(model="gpt-5-nano", use_responses_api=True)
    
    elif provider == "anthropic":
        from langchain_anthropic import ChatAnthropic
        return ChatAnthropic(model="claude-3-5-sonnet-20241022")
    
    elif provider == "ollama":
        from langchain_ollama import ChatOllama
        return ChatOllama(model="llama3.2", format="json")
    
    else:
        raise ValueError(f"Provider inconnu: {provider}")

# Dans vos agents
llm = create_llm()
adapter = LLMAdapter(llm)
```

---

## ğŸ“ Utilisation dans les Agents

### Exemple : CorrectorAgent

```python
from pydantic import BaseModel, Field
from typing import List

class Correction(BaseModel):
    type: str
    original: str
    corrected: str
    explanation: str

class CorrectionResult(BaseModel):
    corrected_text: str
    corrections: List[Correction]
    summary: str

class CorrectorAgent:
    def __init__(self, llm):
        self.adapter = LLMAdapter(llm)
    
    def process(self, text: str) -> CorrectionResult:
        prompt = f"Corrige ce texte: {text}"
        
        # Fonctionne avec TOUS les fournisseurs
        result = self.adapter.get_structured_output(
            prompt=prompt,
            schema=CorrectionResult
        )
        
        return result
```

---

## ğŸ§ª Tests Multi-Fournisseur

```bash
# Tester avec OpenAI
LLM_PROVIDER=openai python examples/corrector_structured_example.py

# Tester avec Ollama (local)
LLM_PROVIDER=ollama python examples/corrector_structured_example.py

# Tester avec Anthropic
LLM_PROVIDER=anthropic python examples/corrector_structured_example.py
```

---

## ğŸ“Š Comparaison des Fournisseurs

| Fournisseur | QualitÃ© | Vitesse | Prix | Structured | Local |
|-------------|---------|---------|------|------------|-------|
| **OpenAI GPT-5-nano** | â­â­â­â­â­ | âš¡âš¡âš¡ | ğŸ’° | âœ… Natif | âŒ |
| **Anthropic Claude** | â­â­â­â­â­ | âš¡âš¡ | ğŸ’°ğŸ’° | âœ… JSON | âŒ |
| **Mistral Large** | â­â­â­â­ | âš¡âš¡âš¡ | ğŸ’° | âš ï¸ JSON | âŒ |
| **Ollama llama3.2** | â­â­â­ | âš¡ | ğŸ†“ | âš ï¸ JSON | âœ… |

---

## ğŸ’¡ Recommandations

### Production
- **OpenAI GPT-5-nano** : Meilleur rapport qualitÃ©/prix/vitesse
- Structured Outputs garantis, pas de parsing

### DÃ©veloppement Local
- **Ollama** : Gratuit, illimitÃ©, privÃ©
- QualitÃ© suffisante pour les tests

### Gros Volumes
- **Mistral** : Bon prix, franÃ§ais natif
- Moins cher que Claude

### Maximum QualitÃ©
- **Claude 3.5 Sonnet** : Meilleur en comprÃ©hension
- Plus cher mais excellent

---

## ğŸ”’ SÃ©curitÃ© & ConfidentialitÃ©

### DonnÃ©es Sensibles
â¡ï¸ **Utilisez Ollama local** : Aucune donnÃ©e ne quitte votre machine

### Production Publique
â¡ï¸ **OpenAI ou Anthropic** : Infrastructure fiable, SLA garantis

### ConformitÃ© RGPD
â¡ï¸ **Mistral (EU)** : HÃ©bergÃ© en Europe, conforme RGPD

---

## ğŸš€ Migration entre Fournisseurs

GrÃ¢ce Ã  `LLMAdapter`, la migration est **sans modification de code** :

```python
# Avant (OpenAI uniquement)
llm = ChatOpenAI(model="gpt-5-nano")

# AprÃ¨s (n'importe quel provider)
llm = create_llm()  # Lit LLM_PROVIDER depuis .env

# Le reste du code reste IDENTIQUE
adapter = LLMAdapter(llm)
result = adapter.get_structured_output(...)
```

---

## ğŸ“š Ressources

- [LangChain Providers](https://python.langchain.com/docs/integrations/chat/)
- [OpenAI Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs)
- [Ollama Models](https://ollama.com/library)
- [Anthropic Claude](https://www.anthropic.com/claude)
- [Mistral AI](https://mistral.ai/)

